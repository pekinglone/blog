<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[K8s折腾日记(二)--在Kubernetes中部署一个单实例有状态的应用]]></title>
    <url>%2Fblog%2F2018%2F09%2F16%2Fspringboot-k8s-2%2F</url>
    <content type="text"><![CDATA[之前已经尝试在Kubernetes中部署了一个简单的Spring boot应用，但这个应用是无状态的，这次主要折腾一下如何运行一个单实例有状态应用(MySQL) 0x01 定义持久化磁盘Kubernetes 中使用 Volume 来持久化保存容器的数据，Volume 的生命周期独立于容器， Pod 中的容器可能频繁的被销毁和重建，但 Volume 会被保留。 从本质上来将， Kubernetes Volume 是一个目录，但 Volume 提供了对各种backend 的抽象，容器使用 Volume 时不用关心数据到底是存在本地节点的文件系统中还是类似 EBS 这种云硬盘中。 接下来，就通过 PersistentVolume 定义一个持久化磁盘： 为了方便，使用 hostPath 这种 Volume，这种类型是映射到主机上目录的卷，应该只用于测试目的或者单节点集群。 新建 local-volume.yaml 123456789101112apiVersion: v1kind: PersistentVolumemetadata: name: mysql-pvspec: capacity: storage: 5Gi volumeMode: Filesystem accessModes: - ReadWriteOnce hostPath: path: /data/mysql 创建持久卷： 1kubectl create -f local-volume.yaml 随后就能通过 kubectl get pv 看到刚创建好的 PV 了。 0x02 创建 MySQL 密码 Secret在 Kubernetes 中可以通过 Secret 对象来保存类似于数据库用户名、密码或者密钥等敏感信息。Secret 会以密文的方式存储数据，避免了直接在配置文件中保存敏感信息，且 Secret 会以 Volume 的形式 mount 到 Pod，这样容器便可以使用这些数据了。 接下来通过yaml文件来创建数据库的 Secret，yaml文件中的敏感数据必须是通过 base64 编码后的结果。 1234root@k8s:~# echo -n root | base64cm9vdA==root@k8s:~# echo -n 123456 | base64MTIzNDU2 新建 mysql-secret.yaml 12345678apiVersion: v1kind: Secretmetadata: name: mysql-passtype: Opaquedata: username: cm9vdA== password: MTIzNDU2 创建 Secret 1kubectl create -f mysql-secret.yaml 随后便能通过 kubectl get secret 查看刚才创建的 Secret 0x03 部署 MySQLPersistentVolume (PV) 是外部存储系统的一块存储空间，在 Kubernetes 中，可通过 PersistentVolumeClaim (PVC) 来申请现已存在的 PV 的使用。 接下来通过yaml文件部署 MySQL： 新建 mysql-deployment.yaml 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162apiVersion: v1kind: Servicemetadata: name: mysql labels: app: mysqlspec: ports: - port: 3306 selector: app: mysql clusterIP: None---apiVersion: v1kind: PersistentVolumeClaimmetadata: name: mysql-pv-claim labels: app: mysqlspec: accessModes: - ReadWriteOnce resources: requests: storage: 5Gi---apiVersion: apps/v1kind: Deploymentmetadata: name: mysql labels: app: mysqlspec: selector: matchLabels: app: mysql strategy: type: Recreate template: metadata: labels: app: mysql spec: containers: - image: mysql:5.6 name: mysql env: - name: MYSQL_ROOT_PASSWORD valueFrom: secretKeyRef: name: mysql-pass key: password ports: - containerPort: 3306 name: mysql volumeMounts: - name: mysql-persistent-storage mountPath: /var/lib/mysql volumes: - name: mysql-persistent-storage persistentVolumeClaim: claimName: mysql-pv-claim 部署 MySQL 1kubectl create -f mysql-deployment.yaml 随后便能通过 kubectl get pod 查看到部署的mysql了。 0x04 访问 MySQL 实例运行MySQL客户端以连接到服务器: 1kubectl run -it --rm --image=mysql:5.6 mysql-client -- mysql -h &lt;pod-ip&gt; -p&lt;password&gt; 这个命令在集群内创建一个新的Pod并运行MySQL客户端,并通过服务将其连接到服务器.如果连接成功,就知道有状态的MySQL database正处于运行状态. root@k8s:~/k8s# kubectl run -it --rm --image=mysql:5.6 mysql-client -- mysql -h mysql -p123456 If you don&apos;t see a command prompt, try pressing enter. mysql&gt; 至此，单实例的 MySQL 就在 Kubernetes 中部署成功了。 0x05 参考资料 运行一个单实例有状态应用 基于 Persistent Volumes 搭建 WordPress 和 MySQL 应用]]></content>
      <categories>
        <category>Cloud Computing</category>
      </categories>
      <tags>
        <tag>kubernetes</tag>
        <tag>mysql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[K8s折腾日记(一)--在Kubernetes中部署spring boot应用]]></title>
    <url>%2Fblog%2F2018%2F09%2F13%2Fspringboot-k8s-1%2F</url>
    <content type="text"><![CDATA[基于容器的微服务架构目前已经成为了开发应用系统的主流，Kubernetes 则是运行微服务应用的理想平台。基于没事儿瞎折腾的态度，自己最近有空闲时间也开始鼓捣起了k8s，经过一步步摸索，终于完成安装和部署。接下来就先分享一下怎么在 kubernetes 中部署一个简单 Spring boot 的应用。 0x00 环境准备 minikube gradle minikube 的安装可以参考使用minikube安装k8s单节点集群 0x01 构建 Spring boot 应用首先，新建一个Spring Boot应用，姑且命名为k8s-service，这个应用就提供一个简单的接口，便于验证是否部署成功。 MainClass: 123456789101112package com.tomoyadeng.demo.springboot.k8s.service;import org.springframework.boot.SpringApplication;import org.springframework.boot.autoconfigure.SpringBootApplication;@SpringBootApplicationpublic class K8sServiceApplication &#123; public static void main(String[] args) &#123; SpringApplication.run(K8sServiceApplication.class, args); &#125;&#125; RestController: 123456789101112package com.tomoyadeng.demo.springboot.k8s.service.controller;import org.springframework.web.bind.annotation.GetMapping;import org.springframework.web.bind.annotation.RestController;@RestControllerpublic class HelloController &#123; @GetMapping("/hello") public String hello() &#123; return "Hello, Kubernetes!"; &#125;&#125; 接下来写个测试用例，在本地跑一下： 123456789101112131415161718192021222324252627282930package com.tomoyadeng.demo.springboot.k8s.service;import org.junit.Test;import org.junit.runner.RunWith;import org.springframework.beans.factory.annotation.Autowired;import org.springframework.boot.test.autoconfigure.web.servlet.AutoConfigureMockMvc;import org.springframework.boot.test.context.SpringBootTest;import org.springframework.http.MediaType;import org.springframework.test.context.junit4.SpringRunner;import org.springframework.test.web.servlet.MockMvc;import org.springframework.test.web.servlet.request.MockMvcRequestBuilders;import static org.hamcrest.Matchers.equalTo;import static org.springframework.test.web.servlet.result.MockMvcResultMatchers.content;import static org.springframework.test.web.servlet.result.MockMvcResultMatchers.status;@RunWith(SpringRunner.class)@SpringBootTest@AutoConfigureMockMvcpublic class K8sServiceApplicationTest &#123; @Autowired private MockMvc mvc; @Test public void getHello() throws Exception &#123; mvc.perform(MockMvcRequestBuilders.get("/hello").accept(MediaType.APPLICATION_JSON)) .andExpect(status().isOk()) .andExpect(content().string(equalTo("Hello, Kubernetes!"))); &#125;&#125; 完整代码参考Github 0x02 将应用打包成 Docker 镜像接下来就需要把 Spring boot 应用打包成 docker 镜像了，这样才能在 kubernetes 中运行。将 Spring boot 应用打包成docker镜像可以选择通过 Dockerfile 的方式，或者借助构建工具进行构建。我这里选择借助 gradle 来构建 docker 镜像(使用se.transmode.gradle:gradle-docker:1.2 插件)，在 build.gradle 定义构建docker镜像的任务： 123456789101112131415161718docker &#123; baseImage "openjdk:8-slim" maintainer 'tomoyadeng@gmail.com'&#125;task buildDocker(type: Docker, dependsOn: build) &#123; applicationName = bootJar.baseName tagVersion = bootJar.version doFirst &#123; copy &#123; from bootJar into stageDir &#125; &#125; volume "/tmp" addFile("$&#123;bootJar.baseName&#125;-$&#123;bootJar.version&#125;.jar", "app.jar") entryPoint(Arrays.asList("java", "-Djava.security.egd=file:/dev/./urandom", "-Dspring.profiles.active=docker", "-jar", "/app.jar"))&#125; 然后执行 gradle build buildDocker 就能构建出docker镜像，构建完成后可以通过 docker images 查看。 0x03 将镜像部署到kubernetes中创建 DeploymentKubernetes Pod 是由一个或多个容器为了管理和联网的目的而绑定在一起构成的组。Deployment 是管理 Pod 创建和伸缩的推荐方法。要部署刚才构建好的容器镜像，首先要创建 Deployment。 新建 k8s-service.yaml 文件 12345678910111213141516171819202122apiVersion: apps/v1beta1kind: Deploymentmetadata: name: k8s-service labels: app: k8s-servicespec: replicas: 1 selector: matchLabels: app: k8s-service template: metadata: labels: app: k8s-service spec: containers: - name: k8s-service image: com.tomoyadeng/k8s-service:1.0 imagePullPolicy: IfNotPresent ports: - containerPort: 8080 运行 kubectl create -f k8s-service.yaml 创建 Deployment，创建完成后可以通过 kubectl get deployments 查看 deployment，通过 kubectl get pods 查看 pod。 创建 Service默认情况下，Pod 只能通过 Kubernetes 集群中的内部 IP 地址访问。要使得 k8s-service 容器可以从 Kubernetes 虚拟网络的外部访问，您必须将 Pod 暴露为 Kubernetes Service。 1kubectl expose deployment k8s-service --type=LoadBalancer 随后可以通过 curl $(minikube service k8s-service --url)/hello 验证部署是否成功。部署的pod的日志可以通过 kubectl log &lt;Pod Name&gt; 来查看。 完整代码参考Github 0x04 参考资料 你好 Minikube]]></content>
      <categories>
        <category>Cloud Computing</category>
      </categories>
      <tags>
        <tag>spring boot</tag>
        <tag>kubernetes</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[一致性哈希算法的一次实践]]></title>
    <url>%2Fblog%2F2018%2F06%2F09%2Fconsistent-hashing-practice%2F</url>
    <content type="text"><![CDATA[最近在分析一个需求，需要开发一个采集器的调度框架，实现采集器的注册，离线以及采集任务分配(负载均衡)。 采集器用于登录到网络设备上采集数据，部分运营商考虑到设备性能问题，会限制同时只能有一个用户登录设备查询数据。那么在此限制下，分配采集任务时需要保证： 对于同一设备的任务始终都落在同一个采集器上去执行，才能保证同一时刻对于同一设备不会有多个采集器采集。 而且，需要保证在某个采集器失效离线时，之前落在该采集器上的设备列表需要均匀分布到剩下的采集器上去，不至于造成某一个采集器负载过大。 到这里，实现方案已经呼之欲出，这不就是解决分布式缓存问题的套路么 — 一致性哈希算法，可以参考这篇文章进行了解 《一致性哈希算法及其在分布式系统中的应用》 0x01 接口定义首先，对几个关键角色进行接口抽象 网络设备12345678910/** * 网络设备 */public interface Device &#123; /** * 获取设备IP * @return IP */ String getIp();&#125; 采集器123456789101112131415161718192021public interface Collector &#123; /** * 获取采集器IP * @return IP */ String getIp(); /** * 设置采集器IP * @param ip IP */ void setIp(String ip); /** * 执行采集任务 * @param device 采集对象--设备 * @param commands 采集命令 * @return 采集结果 */ Map&lt;String, Object&gt; collect(Device device, List&lt;String&gt; commands);&#125; 集群这里把采集器的调度框架抽象成集群，并且使用泛型来定义集群接口 1234567891011121314151617181920212223/** * 调度框架(可看作集群管理) */public interface Cluster&lt;T&gt; &#123; /** * 注册 * @param t */ void register(T t); /** * 离线 * @param t */ void offline(T t); /** * 负载均衡 * @param ip 源IP * @return T */ T choose(String ip);&#125; 0x02 算法实现Hash算法选择在选择设备对应的采集器时，需要对设备的IP进行hash计算。由于设备的IP前缀基本一致，使用默认的字符串hash算法会导致计算出来的hash值不够离散，只能落在hash环上很小的一段区间。因此需要重新选择一种hash算法，保证字符串hash的离散性。这里使用FNV1_32_HASH算法 1234567891011121314151617181920212223/** * FNV1_32_HASH * * @param str str * @return hash */private static int rehash(String str) &#123; final int p = 16777619; int hash = (int) 2166136261L; for (int i = 0; i &lt; str.length(); i++) &#123; hash = (hash ^ str.charAt(i)) * p; &#125; hash += hash &lt;&lt; 13; hash ^= hash &gt;&gt; 7; hash += hash &lt;&lt; 3; hash ^= hash &gt;&gt; 17; hash += hash &lt;&lt; 5; if (hash &lt; 0) &#123; hash = Math.abs(hash); &#125; return hash;&#125; 具体实现123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475/** @author tomoya */public class CollectorCluster implements Cluster&lt;Collector&gt; &#123; /** 每个采集器定义虚拟节点个数 */ private static final int VIRTUAL_NODE_NUMBER = 320; /** 采集器--所有虚拟节点hash值数组 映射关系 */ @GuardedBy("clusterLock") private Map&lt;Collector, int[]&gt; registeredServers = new HashMap&lt;&gt;(); /** hash环上的hash值--采集器 映射关系 */ @GuardedBy("clusterLock") private TreeMap&lt;Integer, Collector&gt; hashRingMap = new TreeMap&lt;&gt;(); private ReadWriteLock clusterLock = new ReentrantReadWriteLock(); @Override public void register(Collector collector) &#123; System.out.println("add server " + collector.toString()); final Lock lock = clusterLock.writeLock(); lock.lock(); try &#123; // 计算采集器所有虚拟节点的hash值，并将所有hash值注册到hash环上 int[] nodesHash = new int[VIRTUAL_NODE_NUMBER]; for (int i = 0; i &lt; VIRTUAL_NODE_NUMBER; i++) &#123; int hash = CollectorCluster.rehash(collector.getIp() + ":" + i); nodesHash[i] = hash; hashRingMap.put(hash, collector); &#125; // 保存采集所有虚拟节点的hash值 registeredServers.put(collector, nodesHash); &#125; finally &#123; lock.unlock(); &#125; &#125; @Override public void offline(Collector collector) &#123; System.out.println("delete server " + collector.toString()); final Lock lock = clusterLock.writeLock(); lock.lock(); try &#123; // 将该采集器所有虚拟节点的hash值从hash环上删除 for (int hash : registeredServers.get(collector)) &#123; hashRingMap.remove(hash); &#125; // 删除采集器 registeredServers.remove(collector); &#125; finally &#123; lock.unlock(); &#125; &#125; @Override public Collector choose(String deviceIp) &#123; final int hash = rehash(deviceIp); final Lock lock = clusterLock.readLock(); lock.lock(); try &#123; // 逆时针找映射节点 Map.Entry&lt;Integer, Collector&gt; entry = hashRingMap.floorEntry(hash); Collector collector = entry == null ? hashRingMap.lastEntry().getValue() : entry.getValue(); System.out.println(deviceIp + " --&gt; " + collector); return collector; &#125; finally &#123; lock.unlock(); &#125; &#125;&#125; 0x03 测试首先实现一个具体的采集器类 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950/** * @author tomoya */public class DefaultCollector implements Collector &#123; private String ip; DefaultCollector(String ip) &#123; this.ip = ip; &#125; @Override public String getIp() &#123; return ip; &#125; @Override public void setIp(String ip) &#123; this.ip = ip; &#125; @Override public Map&lt;String, Object&gt; collect(Device ne, List&lt;String&gt; commands) &#123; return new HashMap&lt;&gt;(commands.size()); &#125; @Override public String toString() &#123; return "DefaultCollector&#123;" + "ip='" + ip + '\'' + '&#125;'; &#125; @Override public boolean equals(Object o) &#123; if (this == o) &#123; return true; &#125; if (o == null || getClass() != o.getClass()) &#123; return false; &#125; DefaultCollector that = (DefaultCollector) o; return Objects.equals(ip, that.ip); &#125; @Override public int hashCode() &#123; return CollectorCluster.rehash(ip); &#125;&#125; 然后直接在CollectorCluster类中增加一个main函数来测试 12345678910111213141516171819public static void main(String[] args) &#123; CollectorCluster cluster = new CollectorCluster(); // 注册5个采集器 List.of("192.168.0.1", "192.168.0.2", "192.168.0.3", "192.168.0.4", "192.168.0.5") .stream() .map(DefaultCollector::new) .forEach(cluster::register); String ipPrefix = "136.10.1."; // 20个设备 进行负载均衡 Stream.iterate(1, i -&gt; i + 1).limit(20).map(i -&gt; ipPrefix + i).forEach(cluster::choose); System.out.println("============"); // 离线一个采集器 cluster.offline(new DefaultCollector("192.168.0.5")); // 20个设备 再次进行负载均衡 Stream.iterate(1, i -&gt; i + 1).limit(20).map(i -&gt; ipPrefix + i).forEach(cluster::choose);&#125;]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>Java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[使用Docker快速搭建Kafka开发环境]]></title>
    <url>%2Fblog%2F2018%2F06%2F02%2Fkafka-cluster-in-docker%2F</url>
    <content type="text"><![CDATA[最近准备学习Kafka，于是买了《Kafka权威指南》来看。作为一个初学者，快速搭建起一套可以运行的环境十分重要，跟着该书第2章的安装介绍可以完成在Linux系统下的环境搭建，但是读下来发现步骤还是有点繁多。有没有什么更加快捷的办法搭建一套可以运行的开发环境呢，于是我想到了Docker。2018年了，容器化已经成为了主流，在本地进行开发和测试的时候使用Docker也便于模拟多节点的集群环境。 0x00 前置条件 Docker: 要想使用Docker来启动kafka，开发环境提前装好Docker是必须的，我一般在Ubuntu虚拟机上进行开发测试 Docker Compose: kafka依赖zookeeper，使用docker-compose来管理容器依赖 0x01 Docker镜像要想使用Docker安装Kafka，第一件事当然是去Docker hub上找镜像以及使用方法啦。发现kafka并不像mysql或者redis那样有官方镜像，不过Google一下后发现可以选择知名的三方镜像wurstmeister/kafka wurstmeister/kafka在Github上更新还算频繁，目前使用kafka版本是1.1.0 0x02 安装 参考官方测试用的docker-compose.yml直接在自定义的目录位置新建docker-compose的配置文件 touch ~/docker/kafka/docker-compose.yml 123456789101112131415version: '2.1'services: zookeeper: image: wurstmeister/zookeeper ports: - "2181" kafka: image: wurstmeister/kafka ports: - "9092" environment: KAFKA_ADVERTISED_HOST_NAME: 192.168.5.139 KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181 volumes: - /var/run/docker.sock:/var/run/docker.sock 注意： KAFKA_ADVERTISED_HOST_NAME 需要配置为宿主机的ip docker-compose 启动kafka 1root@ubuntu:~/docker/kafka# docker-compose up -d 启动完之后通过docker ps可以看到启动了一个zookeeper容器和一个kafka容器 启动多个kafka节点，比如3 1root@ubuntu:~/docker/kafka# docker-compose scale kafka=3 如果没什么错误的话，再通过docker ps可以看到启动了一个zookeeper容器和三个kafka容器 0x03 验证 首先进入到一个kafka容器中，例如: kafka_kafka_1 1root@ubuntu:~/docker/kafka# docker exec -it kafka_kafka_1 /bin/bash 创建一个topic并查看，需要指定zookeeper的容器名(这里是kafka_zookeeper_1)，topic的名字为test 123$KAFKA_HOME/bin/kafka-topics.sh --create --topic test --zookeeper kafka_zookeeper_1:2181 --replication-factor 1 --partitions 1$KAFKA_HOME/bin/kafka-topics.sh --list --zookeeper kafka_zookeeper_1:2181 发布消息，输入几条消息后，按^C退出发布 1$KAFKA_HOME/bin/kafka-console-producer.sh --topic=test --broker-list kafka_kafka_1:9092 接受消息 1$KAFKA_HOME/bin/kafka-console-consumer.sh --bootstrap-server kafka_kafka_1:9092 --from-beginning --topic test 如果接收到了发布的消息，那么说明部署正常，可以正式使用了。]]></content>
      <categories>
        <category>Middleware</category>
      </categories>
      <tags>
        <tag>kafka</tag>
        <tag>docker</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[使用 PriorityQueue 求解 Top K 问题]]></title>
    <url>%2Fblog%2F2018%2F05%2F13%2FtopK-with-PriorityQueue%2F</url>
    <content type="text"><![CDATA[0x00 问题描述这两天在看一些面试题的时候，遇到一个问题: 有N(N&gt;&gt;10000)个整数,求出其中的前K个最大的数 在网上搜了下，大概有三种解决思路： 排序：这种方式最好理解，但是时间复杂度较高(使用快排,O(NlogN)) 堆： 维护一个有边界的小顶堆(O(NlogK)) 位图： 理解较为困难 (O(N)) 自己动手试了试第二种思路在Java中的实现(泛型版本) 0x01 Java实现在 Java 中，PriorityQueue 类实现了堆这种数据结构，可以用来求解Top K 问题。 整个算法的思想就是： 通过PriorityQueue实现一个有界的堆，逐个向堆中添加元素，当元素个数超过边界时，淘汰堆顶元素 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253package com.tomoyadeng.javabeginner.interview;import java.util.*;import java.util.stream.Stream;public class TopK&lt;T&gt; &#123; /** * 堆的边界，Top K 问题中的 K */ private final int boundary; /** * 优先队列，用来构造一个有界的堆 */ private final PriorityQueue&lt;T&gt; boundaryHeap; /** * 通过自定义边界 boundary 可以求解 top K 问题 * 通过自定义比较器 comparator 可以控制求解 top K 大 还是 top K 小 * @param boundary 边界 K * @param comparator 数据比较器 */ public TopK(int boundary, Comparator&lt;T&gt; comparator) &#123; this.boundary = boundary; boundaryHeap = new PriorityQueue&lt;&gt;(boundary, comparator); &#125; /** * 求解数据流中的top K， 将结果写入List中 * @param dataStream 数据流 * @param results top K 结果 */ public void topK(Stream&lt;T&gt; dataStream, List&lt;T&gt; results) &#123; dataStream.forEach(this::add); while (!boundaryHeap.isEmpty()) &#123; results.add(boundaryHeap.poll()); &#125; &#125; /** * 向有界堆中添加元素的帮助方法 * @param t 待添加数据 */ private void add(T t) &#123; boundaryHeap.add(t); if (boundaryHeap.size() &gt; boundary) &#123; boundaryHeap.poll(); &#125; &#125;&#125; 0x02 测试直接写一个main函数进行测试 12345678910111213141516171819202122public static void main(String[] args) &#123; // 构造一个 范围为 [0, 2^30] 的 Integer 流，通过limit可以控制大小 final int upLimit = 1 &lt;&lt; 30; Stream&lt;Integer&gt; stream = Stream.generate(Math::random) .map(d -&gt; d * upLimit) .map(d -&gt; (int) Math.round(d)) .limit(100_000_000); // 将 (o1, o2) -&gt; (o1 - o2) 换成 (o1, o2) -&gt; (o2 - o1) 可以求解 top K 小 TopK&lt;Integer&gt; topK = new TopK&lt;&gt;(10, (o1, o2) -&gt; (o1 - o2)); List&lt;Integer&gt; results = new ArrayList&lt;&gt;(); long startTime = System.currentTimeMillis(); topK.topK(stream, results); long endTime = System.currentTimeMillis(); System.out.println("results: " + results); System.out.println("cost: " + (endTime - startTime) / 1000.0);&#125; 1亿数据测试结果： results: [1073741717, 1073741721, 1073741740, 1073741747, 1073741768, 1073741781, 1073741785, 1073741791, 1073741792, 1073741813]cost: 7.656 0x03 优点分析在输入数据流是一个惰性流(不需要一次性将全部数据加载到内存)的情况下，这种方式速度较快且占用最少的内存，内存中只需要维护一个固定大小的堆即可]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>Java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[kubernetes 安装笔记]]></title>
    <url>%2Fblog%2F2018%2F05%2F06%2Fkubernetes-installation%2F</url>
    <content type="text"><![CDATA[基于Kubernetes1.10.2 0x00 前置条件 ubuntu16.04 虚拟机 shadowsocks 科学上网 本机是Win10，通过Vmvare安装好几台ubuntu16.04虚拟机，均使用root账号登陆操作，本机已经可以通过shadowsocks(简称ss)进行科学上网 虚拟机列表 主机名 ip 角色 配置 OS K8s-1 192.168.5.136 master 2U2G Ubuntu16.04 K8s-2 192.168.5.137 node 1U2G Ubuntu16.04 K8s-3 192.168.5.138 node 1U2G Ubuntu16.04 0x01 准备环境(所有虚拟机)ubuntu虚拟机配置apt源国内配置apt源为阿里云的源下载速度会快一些，(shadowsocks设置为PAC模式) 123456789101112131415$ cp -p /etc/apt/sources.list /etc/apt/sources.list.bak$ cat &lt;&lt;-EOF | sudo tee /etc/apt/sources.listdeb http://mirrors.aliyun.com/ubuntu/ xenial main restricted universe multiversedeb http://mirrors.aliyun.com/ubuntu/ xenial-security main restricted universe multiversedeb http://mirrors.aliyun.com/ubuntu/ xenial-updates main restricted universe multiversedeb http://mirrors.aliyun.com/ubuntu/ xenial-proposed main restricted universe multiversedeb http://mirrors.aliyun.com/ubuntu/ xenial-backports main restricted universe multiversedeb-src http://mirrors.aliyun.com/ubuntu/ xenial main restricted universe multiversedeb-src http://mirrors.aliyun.com/ubuntu/ xenial-security main restricted universe multiversedeb-src http://mirrors.aliyun.com/ubuntu/ xenial-updates main restricted universe multiversedeb-src http://mirrors.aliyun.com/ubuntu/ xenial-proposed main restricted universe multiverseEOF$ apt-get update ubuntu虚拟机配置代理进行科学上网 首先要在本机的shadowsocks上使能局域网访问(右键shadowsocks进行选择) 在ubuntu虚拟机上配置全局代理，proxyserveraddr为本机的IP地址，Windows上可使用ipconfig查看，proxyserverport为shadowsocks端口，默认为1080 将下面的内容添加到~/.profile末尾 123456789101112export proxyserveraddr=192.168.99.248export proxyserverport=1080export HTTP_PROXY="http://$proxyserveraddr:$proxyserverport/"export HTTPS_PROXY="https://$proxyserveraddr:$proxyserverport/"export FTP_PROXY="ftp://$proxyserveraddr:$proxyserverport/"export SOCKS_PROXY="socks://$proxyserveraddr:$proxyserverport/"export NO_PROXY="localhost,127.0.0.1,localaddress,.localdomain.com,10.0.0.0/8,192.168.0.0/16"export http_proxy="http://$proxyserveraddr:$proxyserverport/"export https_proxy="https://$proxyserveraddr:$proxyserverport/"export ftp_proxy="ftp://$proxyserveraddr:$proxyserverport/"export socks_proxy="socks://$proxyserveraddr:$proxyserverport/"export no_proxy="localhost,127.0.0.1,localaddress,.localdomain.com,10.0.0.0/8,192.168.0.0/16" 执行下面的命令生效proxy配置 12345678$ source ~/.profile$ cat &lt;&lt;-EOF| sudo tee /etc/apt/apt.confAcquire::http::proxy "http://$proxyserveraddr:$proxyserverport/";Acquire::https::proxy "https://$proxyserveraddr:$proxyserverport/";Acquire::ftp::proxy "ftp://$proxyserveraddr:$proxyserverport/";Acquire::socks::proxy "socks://$proxyserveraddr:$proxyserverport/";EOF 测试proxy是否可用 1$ curl www.google.com 关闭swap编辑/etc/fstab文件，注释掉引用swap的行，保存并重启后输入sudo swapoff -a即可。参考Kubelet/Kubernetes should work with Swap Enabled 0x02 安装Docker(所有虚拟机)配置好docker源版本： 17.03.0~ce-0~ubuntu-xenial 1234567891011121314$ sudo apt-get update$ sudo apt-get install \ apt-transport-https \ ca-certificates \ curl \ software-properties-common$ curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo apt-key add -$ sudo add-apt-repository \ "deb [arch=amd64] https://download.docker.com/linux/ubuntu \ $(lsb_release -cs) \ stable" 安装Docker CE123$ sudo apt-get update$ sudo apt-get install docker-ce=17.03.0~ce-0~ubuntu-xenial 详细信息参考官方文档 0x03 安装Kubernetes(所有虚拟机)配置docker镜像加速参考阿里云镜像加速器 安装kubeadm, kubelet 和 kubectl123456789$ curl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | apt-key add -$ cat &lt;&lt;EOF &gt;/etc/apt/sources.list.d/kubernetes.listdeb http://apt.kubernetes.io/ kubernetes-xenial mainEOF$ apt-get update$ apt-get install -y kubelet kubeadm kubectl 提前pull镜像由于网络原因，我们需要提前拉取k8s初始化需要用到的镜像，并添加对应的k8s.gcr.io标签 123456789101112131415161718192021222324## 拉取镜像docker pull reg.qiniu.com/k8s/kube-apiserver-amd64:v1.10.2docker pull reg.qiniu.com/k8s/kube-controller-manager-amd64:v1.10.2docker pull reg.qiniu.com/k8s/kube-scheduler-amd64:v1.10.2docker pull reg.qiniu.com/k8s/kube-proxy-amd64:v1.10.2docker pull reg.qiniu.com/k8s/etcd-amd64:3.1.12docker pull reg.qiniu.com/k8s/pause-amd64:3.1## 添加Tagdocker tag reg.qiniu.com/k8s/kube-apiserver-amd64:v1.10.2 k8s.gcr.io/kube-apiserver-amd64:v1.10.2docker tag reg.qiniu.com/k8s/kube-scheduler-amd64:v1.10.2 k8s.gcr.io/kube-scheduler-amd64:v1.10.2docker tag reg.qiniu.com/k8s/kube-controller-manager-amd64:v1.10.2 k8s.gcr.io/kube-controller-manager-amd64:v1.10.2docker tag reg.qiniu.com/k8s/kube-proxy-amd64:v1.10.2 k8s.gcr.io/kube-proxy-amd64:v1.10.2docker tag reg.qiniu.com/k8s/etcd-amd64:3.1.12 k8s.gcr.io/etcd-amd64:3.1.12docker tag reg.qiniu.com/k8s/pause-amd64:3.1 k8s.gcr.io/pause-amd64:3.1## 在Kubernetes 1.10 中，增加了CoreDNS，如果使用CoreDNS(默认关闭)，则不需要下面三个镜像。docker pull reg.qiniu.com/k8s/k8s-dns-sidecar-amd64:1.14.10docker pull reg.qiniu.com/k8s/k8s-dns-kube-dns-amd64:1.14.10docker pull reg.qiniu.com/k8s/k8s-dns-dnsmasq-nanny-amd64:1.14.10docker tag reg.qiniu.com/k8s/k8s-dns-sidecar-amd64:1.14.10 k8s.gcr.io/k8s-dns-sidecar-amd64:1.14.10docker tag reg.qiniu.com/k8s/k8s-dns-kube-dns-amd64:1.14.10 k8s.gcr.io/k8s-dns-kube-dns-amd64:1.14.10docker tag reg.qiniu.com/k8s/k8s-dns-dnsmasq-nanny-amd64:1.14.10 k8s.gcr.io/k8s-dns-dnsmasq-nanny-amd64:1.14.10 需要的镜像是/etc/kubernetes/manifests/目录下查看各个yaml文件中汇总得到的 0x04 配置master节点使用kubeadm init初始化master节点1$ kubeadm init --apiserver-advertise-address=192.168.5.136 --kubernetes-version=v1.10.2 --feature-gates=CoreDNS=true --pod-network-cidr=192.168.0.0/16 init 常用主要参数： –kubernetes-version: 指定Kubenetes版本，如果不指定该参数，会从google网站下载最新的版本信息。 –pod-network-cidr: 指定pod网络的IP地址范围，它的值取决于你在下一步选择的哪个网络网络插件，比如我在本文中使用的是Calico网络，需要指定为192.168.0.0/16。 –apiserver-advertise-address: 指定master服务发布的Ip地址，如果不指定，则会自动检测网络接口，通常是内网IP。 –feature-gates=CoreDNS: 是否使用CoreDNS，值为true/false，CoreDNS插件在1.10中提升到了Beta阶段，最终会成为Kubernetes的缺省选项 最终输出： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960root@K8s-1:~# kubeadm init --apiserver-advertise-address=192.168.5.136 --kubernetes-version=v1.10.2 --feature-gates=CoreDNS=true --pod-network-cidr=192.168.0.0/16[init] Using Kubernetes version: v1.10.2[init] Using Authorization modes: [Node RBAC][preflight] Running pre-flight checks. [WARNING FileExisting-crictl]: crictl not found in system pathSuggestion: go get github.com/kubernetes-incubator/cri-tools/cmd/crictl[preflight] Starting the kubelet service[certificates] Generated ca certificate and key.[certificates] Generated apiserver certificate and key.[certificates] apiserver serving cert is signed for DNS names [k8s-1 kubernetes kubernetes.default kubernetes.default.svc kubernetes.default.svc.cluster.local] and IPs [10.96.0.1 192.168.5.136][certificates] Generated apiserver-kubelet-client certificate and key.[certificates] Generated etcd/ca certificate and key.[certificates] Generated etcd/server certificate and key.[certificates] etcd/server serving cert is signed for DNS names [localhost] and IPs [127.0.0.1][certificates] Generated etcd/peer certificate and key.[certificates] etcd/peer serving cert is signed for DNS names [k8s-1] and IPs [192.168.5.136][certificates] Generated etcd/healthcheck-client certificate and key.[certificates] Generated apiserver-etcd-client certificate and key.[certificates] Generated sa key and public key.[certificates] Generated front-proxy-ca certificate and key.[certificates] Generated front-proxy-client certificate and key.[certificates] Valid certificates and keys now exist in "/etc/kubernetes/pki"[kubeconfig] Wrote KubeConfig file to disk: "/etc/kubernetes/admin.conf"[kubeconfig] Wrote KubeConfig file to disk: "/etc/kubernetes/kubelet.conf"[kubeconfig] Wrote KubeConfig file to disk: "/etc/kubernetes/controller-manager.conf"[kubeconfig] Wrote KubeConfig file to disk: "/etc/kubernetes/scheduler.conf"[controlplane] Wrote Static Pod manifest for component kube-apiserver to "/etc/kubernetes/manifests/kube-apiserver.yaml"[controlplane] Wrote Static Pod manifest for component kube-controller-manager to "/etc/kubernetes/manifests/kube-controller-manager.yaml"[controlplane] Wrote Static Pod manifest for component kube-scheduler to "/etc/kubernetes/manifests/kube-scheduler.yaml"[etcd] Wrote Static Pod manifest for a local etcd instance to "/etc/kubernetes/manifests/etcd.yaml"[init] Waiting for the kubelet to boot up the control plane as Static Pods from directory "/etc/kubernetes/manifests".[init] This might take a minute or longer if the control plane images have to be pulled.[apiclient] All control plane components are healthy after 24.001741 seconds[uploadconfig] Storing the configuration used in ConfigMap "kubeadm-config" in the "kube-system" Namespace[markmaster] Will mark node k8s-1 as master by adding a label and a taint[markmaster] Master k8s-1 tainted and labelled with key/value: node-role.kubernetes.io/master=""[bootstraptoken] Using token: dfzpd9.7xxx9kre811fgw1n[bootstraptoken] Configured RBAC rules to allow Node Bootstrap tokens to post CSRs in order for nodes to get long term certificate credentials[bootstraptoken] Configured RBAC rules to allow the csrapprover controller automatically approve CSRs from a Node Bootstrap Token[bootstraptoken] Configured RBAC rules to allow certificate rotation for all node client certificates in the cluster[bootstraptoken] Creating the "cluster-info" ConfigMap in the "kube-public" namespace[addons] Applied essential addon: CoreDNS[addons] Applied essential addon: kube-proxyYour Kubernetes master has initialized successfully!To start using your cluster, you need to run the following as a regular user: mkdir -p $HOME/.kube sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config sudo chown $(id -u):$(id -g) $HOME/.kube/configYou should now deploy a pod network to the cluster.Run "kubectl apply -f [podnetwork].yaml" with one of the options listed at: https://kubernetes.io/docs/concepts/cluster-administration/addons/You can now join any number of machines by running the following on each nodeas root: kubeadm join 192.168.5.136:6443 --token dfzpd9.7xxx9kre811fgw1n --discovery-token-ca-cert-hash sha256:31d53cf706073f557b43e5e7acacb47dceb828ccced3c648dbfbfa4d86d74b1c kubeadm init 输出的token用于master和加入节点间的身份认证，token是机密的，需要保证它的安全，因为拥有此标记的人都可以随意向集群中添加节点。 如果想在非root用户下使用kubectl，可以执行如下命令 123mkdir -p $HOME/.kubesudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/configsudo chown $(id -u):$(id -g) $HOME/.kube/config 验证部署情况在浏览器中输入https://&lt;master-ip&gt;:6443来验证一下是否部署成功，我这里master-ip是192.168.5.136， 返回如下： 1234567891011121314&#123; "kind": "Status", "apiVersion": "v1", "metadata": &#123; &#125;, "status": "Failure", "message": "forbidden: User \"system:anonymous\" cannot get path \"/\"", "reason": "Forbidden", "details": &#123; &#125;, "code": 403&#125; 安装网络插件安装一个网络插件是必须的，因为你的pods之间需要彼此通信。 网络部署必须是优先于任何应用的部署，详细的网络列表可参考插件页面。 本文使用的是Calico网络，安装如下： 12# 使用国内镜像kubectl apply -f http://mirror.faasx.com/kubernetes/installation/hosted/kubeadm/1.7/calico.yaml 插件安装完成后，可以通过检查coredns pod的运行状态来判断网络插件是否正常运行 1234567891011121314root@K8s-1:~# kubectl get pods --all-namespacesNAMESPACE NAME READY STATUS RESTARTS AGEkube-system calico-etcd-nmj5j 1/1 Running 0 54mkube-system calico-kube-controllers-f9d6c4cb6-kxlb2 1/1 Running 0 54mkube-system calico-node-85nm9 2/2 Running 0 13mkube-system calico-node-vrhdl 2/2 Running 0 54mkube-system coredns-7997f8864c-2mp87 1/1 Running 0 59mkube-system coredns-7997f8864c-2vl6h 1/1 Running 0 59mkube-system etcd-k8s-1 1/1 Running 0 58mkube-system kube-apiserver-k8s-1 1/1 Running 0 58mkube-system kube-controller-manager-k8s-1 1/1 Running 0 58mkube-system kube-proxy-48p2g 1/1 Running 0 13mkube-system kube-proxy-5chhg 1/1 Running 0 59mkube-system kube-scheduler-k8s-1 1/1 Running 0 58m 等待coredns pod的状态变成Running，就可以继续添加从节点了。 0x05 添加从节点在从节点上按照前面的步骤按照好docker和kubeadm后，就可以添加从节点到主节点上了 1kubeadm join 192.168.5.136:6443 --token dfzpd9.7xxx9kre811fgw1n --discovery-token-ca-cert-hash sha256:31d53cf706073f557b43e5e7acacb47dceb828ccced3c648dbfbfa4d86d74b1c 完整输出如下： 12345678910111213141516root@K8s-2:~# kubeadm join 192.168.5.136:6443 --token dfzpd9.7xxx9kre811fgw1n --discovery-token-ca-cert-hash sha256:31d53cf706073f557b43e5e7acacb47dceb828ccced3c648dbfbfa4d86d74b1c[preflight] Running pre-flight checks. [WARNING FileExisting-crictl]: crictl not found in system pathSuggestion: go get github.com/kubernetes-incubator/cri-tools/cmd/crictl[discovery] Trying to connect to API Server "192.168.5.136:6443"[discovery] Created cluster-info discovery client, requesting info from "https://192.168.5.136:6443"[discovery] Requesting info from "https://192.168.5.136:6443" again to validate TLS against the pinned public key[discovery] Cluster info signature and contents are valid and TLS certificate validates against pinned roots, will use API Server "192.168.5.136:6443"[discovery] Successfully established connection with API Server "192.168.5.136:6443"This node has joined the cluster:* Certificate signing request was sent to master and a response was received.* The Kubelet was informed of the new secure connection details.Run 'kubectl get nodes' on the master to see this node join the cluster. 过一会儿就可以通过kubectl get nodes命令在主节点上查询到从节点了 12345root@K8s-1:~# kubectl get nodesNAME STATUS ROLES AGE VERSIONk8s-1 Ready master 1h v1.10.2k8s-2 Ready &lt;none&gt; 1m v1.10.2k8s-3 Ready &lt;none&gt; 23m v1.10.2 0x06 卸载集群想要撤销kubeadm做的事，首先要排除节点，并确保在关闭节点之前要清空节点。 在主节点上运行： 12kubectl drain &lt;node name&gt; --delete-local-data --force --ignore-daemonsetskubectl delete node &lt;node name&gt; 然后在需要移除的节点上，重置kubeadm的安装状态： 1kubeadm reset 如果你想重新配置集群，只需运行kubeadm init或者kubeadm join并使用所需的参数即可 参考资料 Installing kubeadm Using kubeadm to Create a Cluster 使用kubeadm搭建Kubernetes(1.10.2)集群（国内环境）]]></content>
      <categories>
        <category>Cloud Computing</category>
      </categories>
      <tags>
        <tag>kubernetes</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[spring-boot-mybatis-docker整合使用]]></title>
    <url>%2Fblog%2F2017%2F07%2F23%2Fspring-boot-mybatis-docker%2F</url>
    <content type="text"><![CDATA[0x00 前置条件 Java Maven Docker, Docker Compose 0x01 使用maven新建Spring Boot工程按工程根目录的相对路径创建如下文件 pom.xml12345678910111213141516171819202122232425262728293031323334353637&lt;?xml version="1.0" encoding="UTF-8"?&gt;&lt;project xmlns="http://maven.apache.org/POM/4.0.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd"&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;groupId&gt;com.tomoyadeng&lt;/groupId&gt; &lt;artifactId&gt;demo-springboot-docker&lt;/artifactId&gt; &lt;version&gt;1.0.0&lt;/version&gt; &lt;parent&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-parent&lt;/artifactId&gt; &lt;version&gt;1.5.4.RELEASE&lt;/version&gt; &lt;/parent&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-test&lt;/artifactId&gt; &lt;scope&gt;test&lt;/scope&gt; &lt;/dependency&gt; &lt;/dependencies&gt; &lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-maven-plugin&lt;/artifactId&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/build&gt;&lt;/project&gt; src/main/java/demo/Application.java12345678910111213141516171819package demo;import org.springframework.boot.SpringApplication;import org.springframework.boot.autoconfigure.SpringBootApplication;import org.springframework.web.bind.annotation.RequestMapping;import org.springframework.web.bind.annotation.RestController;@SpringBootApplication@RestControllerpublic class Application &#123; @RequestMapping("/") public String home() &#123; return "Get started"; &#125; public static void main(String[] args) &#123; SpringApplication.run(Application.class, args); &#125;&#125; 这样，一个简单的Spring boot的应用就创建OK。可使用mvn package编译打包为jar， 然后使用命令行java -jar target/demo-springboot-docker-1.0.0.jar直接启动 0x02 将应用docker化首先创建应用的Dockerfile src/main/docker/Dockerfile123456FROM java:8VOLUME /tmpADD demo-springboot-docker-1.0.0.jar app.jarRUN sh -c 'touch /app.jar'ENV JAVA_OPTS=""ENTRYPOINT [ "sh", "-c", "java $JAVA_OPTS -Djava.security.egd=file:/dev/./urandom -jar /app.jar" ] 然后在pom.xml中添加maven插件依赖，以支持构建docker镜像 pom.xml 12345678910111213141516&lt;plugin&gt; &lt;groupId&gt;com.spotify&lt;/groupId&gt; &lt;artifactId&gt;docker-maven-plugin&lt;/artifactId&gt; &lt;version&gt;0.4.11&lt;/version&gt; &lt;configuration&gt; &lt;imageName&gt;tomoyadeng/$&#123;project.artifactId&#125;&lt;/imageName&gt; &lt;dockerDirectory&gt;src/main/docker&lt;/dockerDirectory&gt; &lt;resources&gt; &lt;resource&gt; &lt;targetPath&gt;/&lt;/targetPath&gt; &lt;directory&gt;$&#123;project.build.directory&#125;&lt;/directory&gt; &lt;include&gt;$&#123;project.build.finalName&#125;.jar&lt;/include&gt; &lt;/resource&gt; &lt;/resources&gt; &lt;/configuration&gt;&lt;/plugin&gt; 使用mvn install docker:build即可构建docker镜像，构建完成后，docker images可查看当前的镜像。 docker run -p 8080:8080 -t tomoyadeng/demo-springboot-docker 可以启动docker容器，此时就完成了此应用的docker化1234567891011tomoya@ubuntu:~/Code/demo-springboot-docker$ docker run -p 8080:8080 -t tomoyadeng/demo-springboot-docker . ____ _ __ _ _ /\\ / ___'_ __ _ _(_)_ __ __ _ \ \ \ \( ( )\___ | '_ | '_| | '_ \/ _` | \ \ \ \ \\/ ___)| |_)| | | | | || (_| | ) ) ) ) ' |____| .__|_| |_|_| |_\__, | / / / / =========|_|==============|___/=/_/_/_/ :: Spring Boot :: (v1.5.4.RELEASE)2017-07-23 11:53:34.894 INFO 5 --- [ main] demo.Application : Starting Application v1.0.0 on f1ff304f4b94 with PID 5 (/app.jar started by root in /)... 可使用docker stop和docker rm来停止容器运行 12345tomoya@ubuntu:~/Code/demo-springboot-docker$ docker psCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMESf1ff304f4b94 tomoyadeng/demo-springboot-docker "sh -c 'java $JAVA..." About a minute ago Up About a minute 0.0.0.0:8080-&gt;8080/tcp keen_swartz$ docker stop f1ff304f4b94$ docker rm f1ff304f4b94 0x03 创建MyBatis的demo首先，在pom.xml中添加mybatis和mysql-connector的依赖 pom.xml12345678910&lt;dependency&gt; &lt;groupId&gt;org.mybatis.spring.boot&lt;/groupId&gt; &lt;artifactId&gt;mybatis-spring-boot-starter&lt;/artifactId&gt; &lt;version&gt;1.3.0&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;mysql&lt;/groupId&gt; &lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt; &lt;version&gt;6.0.6&lt;/version&gt;&lt;/dependency&gt; 添加object类，此处省略了getter和setter src/main/java/demo/domain/User.java123456789101112131415package demo.domain;import java.io.Serializable;public class User implements Serializable &#123; private static final long serialVersionUID = 1L; private int id; private String name; private String phoneNo; private String email;&#125; 添加mapper src/main/java/demo/mapper/UserMapper.java123456789101112package demo.mapper;import demo.domain.User;import org.apache.ibatis.annotations.Mapper;import org.apache.ibatis.annotations.Param;import org.apache.ibatis.annotations.Select;@Mapperpublic interface UserMapper &#123; @Select("select * from tbl_user where name = #&#123;name&#125;") User findByName(@Param("name") String name);&#125; 修改Application.java，添加查询数据库的操作 src/main/java/demo/Application.java123456789101112131415161718192021222324252627282930313233343536package demo;import demo.domain.User;import demo.mapper.UserMapper;import org.springframework.boot.SpringApplication;import org.springframework.boot.autoconfigure.SpringBootApplication;import org.springframework.web.bind.annotation.RequestMapping;import org.springframework.web.bind.annotation.RequestParam;import org.springframework.web.bind.annotation.RestController;import static org.springframework.web.bind.annotation.RequestMethod.GET;@SpringBootApplication@RestControllerpublic class Application &#123; final private UserMapper userMapper; public Application(UserMapper userMapper) &#123; this.userMapper = userMapper; &#125; @RequestMapping("/") public String home() &#123; return "Get started"; &#125; @RequestMapping(value = "/user", method = GET) public String getUserByName(@RequestParam("name") String name) &#123; User user = this.userMapper.findByName(name); return user == null ? "No such user!" : user.toString(); &#125; public static void main(String[] args) &#123; SpringApplication.run(Application.class, args); &#125;&#125; 添加application.yml配置文件 src/main/resources/application.yml1234567891011121314151617# application.ymlspring: datasource: url: jdbc:mysql://localhost:3306/mybatis?useUnicode=true&amp;useJDBCCompliantTimezoneShift=true&amp;useLegacyDatetimeCode=false&amp;serverTimezone=UTC&amp;useSSL=false driverClassName: com.mysql.cj.jdbc.Driver username: root password: 123456 schema: classpath:schema.sql---spring: profiles: container datasource: url: jdbc:mysql://$&#123;DATABASE_HOST&#125;:$&#123;DATABASE_PORT&#125;/$&#123;DATABASE_NAME&#125;?useUnicode=true&amp;useJDBCCompliantTimezoneShift=true&amp;useLegacyDatetimeCode=false&amp;serverTimezone=UTC&amp;useSSL=false username: $&#123;DATABASE_USER&#125; password: $&#123;DATABASE_PASSWORD&#125; schema: classpath:schema.sql initialize: true 附：schema.sql src/main/resources/schema.sql12345drop table if exists tbl_user;create table tbl_user(id int primary key auto_increment,name varchar(32),phoneNo varchar(16), email varchar(32));insert into tbl_user(name, phoneNo, email) values ('dave', '13012345678', 'dave@tomoyadeng.com'); 修改Dockerfile，主要是修改ENTRYPOINT src/main/docker/Dockerfile123456FROM java:8VOLUME /tmpADD demo-springboot-docker-1.0.0.jar app.jarRUN sh -c 'touch /app.jar'ENV JAVA_OPTS=""ENTRYPOINT ["java","-Djava.security.egd=file:/dev/./urandom","-Dspring.profiles.active=container","-jar","/app.jar"] 0x04 手动启动docker应用首先，我们需要先启动一个mysql的容器，执行下面命令即可 1234567docker run -d \ --name mybatis-mysql \ -e MYSQL_ROOT_PASSWORD=123456 \ -e MYSQL_DATABASE=mybatis \ -e MYSQL_USER=dbuser \ -e MYSQL_PASSWORD=123456 \ mysql:latest 启动完成后，可用docker ps查看，也可以通过执行下面命令连接到mysql 12docker run -it --link mybatis-mysql:mysql --rm mysql sh \ -c 'exec mysql -h"$MYSQL_PORT_3306_TCP_ADDR" -P"$MYSQL_PORT_3306_TCP_PORT" -uroot -p"$MYSQL_ENV_MYSQL_ROOT_PASSWORD"' 然后，启动应用容器并连接到mysql 12345678910docker run -d -t \ --name demo-springboot-docker \ --link mybatis-mysql:mysql \ -p 8088:8080 \ -e DATABASE_HOST=mybatis-mysql \ -e DATABASE_PORT=3306 \ -e DATABASE_NAME=mybatis \ -e DATABASE_USER=root \ -e DATABASE_PASSWORD=123456 \ tomoyadeng/demo-springboot-docker 启动完成后，使用docker ps查看，或者直接访问url测试 1curl http://localhost:8088/user?name=dave 0x05 使用docker-compose启动在项目根路径下增加docker-compose的配置文件 docker-compose.yml 12345678910111213141516171819202122version: '3.3'services: mybatis-mysql: image: mysql:latest environment: - MYSQL_ROOT_PASSWORD=123456 - MYSQL_DATABASE=mybatis - MYSQL_USER=dbuser - MYSQL_PASSWORD=123456 demo-springboot-docker: image: tomoyadeng/demo-springboot-docker depends_on: - mybatis-mysql ports: - 8088:8080 environment: - DATABASE_HOST=mybatis-mysql - DATABASE_USER=root - DATABASE_PASSWORD=123456 - DATABASE_NAME=mybatis - DATABASE_PORT=3306 启动前，先将之前手动启动的容器停掉 1234docker stop demo-springboot-dockerdocker stop mybatis-mysqldocker rm demo-springboot-dockerdocker rm mybatis-mysql 然后直接使用命令启动 1docker-compose up]]></content>
      <categories>
        <category>Framework</category>
      </categories>
      <tags>
        <tag>docker</tag>
        <tag>spring boot</tag>
      </tags>
  </entry>
</search>
